{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a36c9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: groq in ./.venv/lib/python3.13/site-packages (0.29.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./.venv/lib/python3.13/site-packages (from groq) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib seaborn ipywidgets python-dotenv groq scikit-learn\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dcd9ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from groq import Groq\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b4e9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_plot_suggestions(df):\n",
    "    print(\"🧠 [LLM] Generating plot suggestions...\")\n",
    "\n",
    "    column_list = ', '.join(df.columns)\n",
    "    data_sample = df.head(5).to_string(index=False)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert data analyst.\n",
    "\n",
    "The user has uploaded a dataset with the following columns:\n",
    "{', '.join(df.columns)}\n",
    "\n",
    "Your task is to recommend 5 meaningful exploratory data analysis (EDA) plots that can be generated from this dataset.\n",
    "\n",
    "For **each plot**, provide:\n",
    "1. **Chart Type**: e.g., Scatter Plot, Histogram, Box Plot, Stacked Bar Chart, Bar, Heatmap, Line, Line Plot, Pie, Pie Plot etc.\n",
    "2. **Columns**: Mention only existing column names from the dataset. Clearly specify the **X-axis and Y-axis** roles, especially for Scatter or Box plots (e.g., \"X: Pclass, Y: Fare\").\n",
    "3. **Title**: A short but descriptive title for the plot.\n",
    "4. **Explanation**: Explain what this plot reveals and why it’s useful.\n",
    "\n",
    "Use the following format exactly:\n",
    "\n",
    "Chart Type: <Chart Type>  \n",
    "Columns: X: <column1>, Y: <column2>  \n",
    "Title: <plot title>  \n",
    "Explanation: <Detailed explanation of what the plot tells us>\n",
    "\n",
    "Repeat for 5 plots.\n",
    "\n",
    "Make sure:\n",
    "- You only use columns from the dataset.\n",
    "- Before writing chart code, check that the columns you use actually exist in the dataset. If any column doesn't exist, skip or correct it using fuzzy matching from column list: [col1, col2, ...]\n",
    "- For boxplots or bar charts, the X-axis must be **categorical** and the Y-axis **numerical**.\n",
    "- For scatter plots, both axes must be **numerical**.\n",
    "\"\"\" \n",
    "\n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        suggestion_text = response.choices[0].message.content\n",
    "        display(HTML(\"<h4>📊 Suggested Plot Ideas from LLM:</h4>\"))\n",
    "        print(suggestion_text)\n",
    "        globals()[\"plot_suggestions_text\"] = suggestion_text\n",
    "        return suggestion_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM Error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1fc3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_columns_to_df(raw_columns, df_columns):\n",
    "    matched = []\n",
    "\n",
    "    for raw_col in raw_columns:\n",
    "        cleaned = raw_col.strip().lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        best_match = None\n",
    "\n",
    "        for actual_col in df_columns:\n",
    "            normalized = actual_col.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "            if normalized == cleaned:\n",
    "                best_match = actual_col\n",
    "                break\n",
    "\n",
    "        if best_match:\n",
    "            matched.append(best_match)\n",
    "        else:\n",
    "            matched.append(raw_col)  # fallback if no match found\n",
    "\n",
    "    return matched\n",
    "\n",
    "def extract_plot_instructions(suggestion_text):\n",
    "    import re\n",
    "    print(\"🧾 Parsing LLM suggestions...\")\n",
    "\n",
    "    instructions = []\n",
    "\n",
    "    chart_type_map = {\n",
    "    \"scatter plot\": \"scatter\",\n",
    "    \"scatter\": \"scatter\",\n",
    "    \"bar chart\": \"bar\",\n",
    "    \"bar\": \"bar\",\n",
    "    \"stacked bar chart\": \"stacked_bar\",\n",
    "    \"stacked bar\": \"stacked_bar\",\n",
    "    \"stacked histogram\": \"histogram\",   # normalize here\n",
    "    \"histogram\": \"histogram\",\n",
    "    \"box plot\": \"boxplot\",\n",
    "    \"boxplot\": \"boxplot\",\n",
    "    \"line\": \"line\",\n",
    "    \"pie\": \"pie\",\n",
    "    \"heatmap\": \"heatmap\",\n",
    "    \"pairplot\": \"pairplot\"\n",
    "}\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'Chart Type:\\s*(.*?)\\s+Columns:\\s*(?:(?:X:\\s*(.*?),\\s*Y:\\s*(.*?))|(.+?))\\s+Title:\\s*(.*?)\\s+Explanation:\\s*(.*?)(?=Chart Type:|$)',\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    matches = pattern.findall(suggestion_text)\n",
    "\n",
    "    print(f\"🔍 Found {len(matches)} suggestions.\")\n",
    "\n",
    "    for idx, match in enumerate(matches, 1):\n",
    "        chart_type_raw = match[0].strip()\n",
    "        x_col = match[1].strip() if match[1] else None\n",
    "        y_col = match[2].strip() if match[2] else None\n",
    "        fallback_cols = match[3].strip() if match[3] else \"\"\n",
    "        title = match[4].strip()\n",
    "        desc = match[5].strip()\n",
    "\n",
    "        # Build unified raw column string\n",
    "        raw_col_string = f\"{x_col}, {y_col}\" if x_col and y_col else fallback_cols\n",
    "\n",
    "        # Clean column names\n",
    "        raw_col_string = re.sub(r\"\\s*\\(.*?\\)\", \"\", raw_col_string)  # Remove parentheses\n",
    "        raw_col_string = re.sub(r\"(?i)(color|hue):\\s*\\w+\", \"\", raw_col_string)  # Remove 'color:' or 'hue:'\n",
    "        raw_col_string = re.sub(r\"(?i)count of \", \"\", raw_col_string)\n",
    "        raw_col_string = re.sub(r\"(?i)count\\((.*?)\\)\", r\"\\1\", raw_col_string)\n",
    "        # Split and normalize\n",
    "        raw_columns = [col.strip() for col in raw_col_string.split(\",\") if col.strip()]\n",
    "        col_list = match_columns_to_df(raw_columns, df.columns)  \n",
    "                  \n",
    "        # 🔎 Identify chart type\n",
    "        chart_type = chart_type_map.get(chart_type_raw.lower(), chart_type_raw.lower())\n",
    "        \n",
    "        instruction = {\n",
    "            'type': chart_type,\n",
    "            'columns': col_list,\n",
    "            'title': title,\n",
    "            'description': desc\n",
    "        }\n",
    "\n",
    "        instructions.append(instruction)\n",
    "        print(f\"✅ {idx}. [{chart_type}] Columns: {col_list} → {title}\")\n",
    "\n",
    "    if not instructions:\n",
    "        print(\"⚠️ No valid instructions extracted from LLM output.\")\n",
    "\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47db6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def parse_column_aliases(col_list, actual_columns):\n",
    "    \"\"\"\n",
    "    Attempts to map aliased column names (e.g., 'Survived and Non-Survived') to actual column names in the dataset.\n",
    "\n",
    "    Args:\n",
    "        col_list (list): List of columns suggested by the LLM.\n",
    "        actual_columns (list): List of real column names from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (resolved_col_list, hue_column if found)\n",
    "    \"\"\"\n",
    "    resolved_cols = []\n",
    "    hue_col = None\n",
    "\n",
    "    for col in col_list:\n",
    "        cleaned = col.lower().strip()\n",
    "\n",
    "        # Handle phrases like \"X and Y\" to infer hue column\n",
    "        if \" and \" in cleaned or \"/\" in cleaned:\n",
    "            # Find a matching real column\n",
    "            potential_tokens = re.split(r\"\\s*(?:and|\\/)\\s*\", cleaned)\n",
    "            for token in potential_tokens:\n",
    "                match = difflib.get_close_matches(token.strip().title(), actual_columns, n=1)\n",
    "                if match:\n",
    "                    hue_col = match[0]\n",
    "                    break\n",
    "            continue\n",
    "\n",
    "        # Direct or fuzzy match\n",
    "        match = difflib.get_close_matches(cleaned.title(), actual_columns, n=1)\n",
    "        if match:\n",
    "            resolved_cols.append(match[0])\n",
    "        else:\n",
    "            # If nothing matches, log for debugging\n",
    "            print(f\"⚠️ Could not match column alias: {col}\")\n",
    "\n",
    "    return resolved_cols, hue_col\n",
    "\n",
    "def resolve_computed_columns(cols, df):\n",
    "    \"\"\"\n",
    "    Detects and computes expressions like 'SibSp + Parch' and adds them\n",
    "    as new temporary columns in the DataFrame. Returns updated column list.\n",
    "    \"\"\"\n",
    "    updated_cols = []\n",
    "    for col in cols:\n",
    "        if '+' in col:\n",
    "            parts = [p.strip() for p in col.split('+')]\n",
    "            if all(p in df.columns for p in parts):\n",
    "                new_col_name = \"_plus_\".join(parts)\n",
    "                if new_col_name not in df.columns:\n",
    "                    df[new_col_name] = df[parts].sum(axis=1)\n",
    "                updated_cols.append(new_col_name)\n",
    "            else:\n",
    "                updated_cols.append(col)  # leave untouched if not all found\n",
    "        else:\n",
    "            updated_cols.append(col)\n",
    "    return updated_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51db8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Plot Helper\n",
    "def save_plot(title):\n",
    "    safe_title = re.sub(r\"[^\\w\\s-]\", \"\", title).strip().replace(\" \", \"_\")\n",
    "    filepath = f\"plots/{safe_title}.png\"\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plt.savefig(filepath)\n",
    "    print(f\"💾 Plot saved to: {filepath}\")\n",
    "\n",
    "# Plot Dispatch Dictionary\n",
    "\n",
    "def boxplot_handler(df, cols, title):\n",
    "    if len(cols) == 2:\n",
    "        cat, num = None, None\n",
    "        for col in cols:\n",
    "            if df[col].nunique() < 10 or df[col].dtype == \"object\":\n",
    "                cat = col\n",
    "            else:\n",
    "                num = col\n",
    "        if cat and num:\n",
    "            sns.boxplot(data=df, x=cat, y=num)\n",
    "        else:\n",
    "            sns.boxplot(data=df, x=cols[0], y=cols[1])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(cat if cat else cols[0])\n",
    "        plt.ylabel(num if num else cols[1])\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def barplot_handler(df, cols, title):\n",
    "    if len(cols) == 1:\n",
    "        sns.countplot(data=df, x=cols[0])\n",
    "    elif len(cols) == 2:\n",
    "        grouped = df.groupby(cols[0])[cols[1]].count()\n",
    "        grouped.plot(kind='bar')\n",
    "        plt.ylabel(f\"Count of {cols[1]}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(cols[0])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def stacked_barplot_handler(df, cols, title):\n",
    "    if len(cols) == 2:\n",
    "        ctab = pd.crosstab(df[cols[0]], df[cols[1]])\n",
    "        ctab.plot(kind='bar', stacked=True)\n",
    "        plt.ylabel(\"Count\")\n",
    "    elif len(cols) == 3:\n",
    "        ctab = pd.crosstab(df[cols[1]], df[cols[2]], values=df[cols[0]], aggfunc='count').fillna(0)\n",
    "        ctab.plot(kind='bar', stacked=True)\n",
    "        plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(cols[0])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def histogram_handler(df, cols, title):\n",
    "    if len(cols) == 1:\n",
    "        col = cols[0]\n",
    "        bins = min(20, df[col].nunique())\n",
    "        sns.histplot(data=df, x=col, bins=bins)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def scatter_handler(df, cols, title):\n",
    "    if len(cols) == 2:\n",
    "        sns.scatterplot(data=df, x=cols[0], y=cols[1])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(cols[0])\n",
    "        plt.ylabel(cols[1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def heatmap_handler(df, cols, title):\n",
    "    try:\n",
    "        if len(cols) >= 2:\n",
    "            pivot_data = df[cols].dropna()\n",
    "            correlation_matrix = pd.crosstab(pivot_data[cols[0]], pivot_data[cols[1]])\n",
    "        else:\n",
    "            # default to correlation heatmap\n",
    "            numeric_df = df.select_dtypes(include=['number'])\n",
    "            correlation_matrix = numeric_df.corr()\n",
    "\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error plotting heatmap: {e}\")\n",
    "\n",
    "def piechart_handler(df, cols, title):\n",
    "    try:\n",
    "        if len(cols) != 1:\n",
    "            print(\"⚠️ Pie chart requires exactly 1 categorical column.\")\n",
    "            return\n",
    "\n",
    "        data = df[cols[0]].value_counts()\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.pie(data.values, labels=data.index, autopct='%1.1f%%', startangle=140)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error plotting pie chart: {e}\")\n",
    "\n",
    "def lineplot_handler(df, cols, title):\n",
    "    try:\n",
    "        if len(cols) == 2:\n",
    "            df_plot = df[cols].dropna().sort_values(by=cols[0])\n",
    "            sns.lineplot(data=df_plot, x=cols[0], y=cols[1])\n",
    "        elif len(cols) == 1:\n",
    "            df_plot = df[cols].dropna()\n",
    "            df_plot.reset_index(drop=True, inplace=True)\n",
    "            df_plot['index'] = df_plot.index\n",
    "            sns.lineplot(data=df_plot, x='index', y=cols[0])\n",
    "        else:\n",
    "            print(\"⚠️ Line plot supports 1 or 2 columns only.\")\n",
    "            return\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error plotting line plot: {e}\")\n",
    "\n",
    "# Mapping chart types to handler functions\n",
    "plot_dispatch = {\n",
    "    \"boxplot\": boxplot_handler,\n",
    "    \"bar\": barplot_handler,\n",
    "    \"stacked_bar\": stacked_barplot_handler,\n",
    "    \"histogram\": histogram_handler,\n",
    "    \"scatter\": scatter_handler,\n",
    "    \"heatmap\": heatmap_handler,\n",
    "    \"line\": lineplot_handler,\n",
    "    \"lineplot\": lineplot_handler,\n",
    "    \"pie\": piechart_handler,\n",
    "    \"piechart\": piechart_handler\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caa2cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots_from_instructions(df, instructions):\n",
    "    print(\"📈 Generating plots from LLM instructions...\")\n",
    "    display(HTML(\"<h4>📈 Generated Plots with Explanations:</h4>\"))\n",
    "\n",
    "    if not instructions:\n",
    "        print(\"⚠️ No plot instructions provided.\")\n",
    "        return\n",
    "\n",
    "    any_plotted = False\n",
    "\n",
    "    for idx, item in enumerate(instructions, 1):\n",
    "        chart_type = item['type']\n",
    "        cols = item['columns']\n",
    "        title = item['title']\n",
    "        explanation = item.get(\"description\", \"\")\n",
    "\n",
    "        print(f\"🔧 [{idx}] Type: {chart_type}, Columns: {cols}, Title: {title}\")\n",
    "\n",
    "        # Fuzzy matching fallback\n",
    "        fallback_cols = []\n",
    "        for col in cols:\n",
    "            if col not in df.columns:\n",
    "                close = difflib.get_close_matches(col, df.columns, n=1)\n",
    "                fallback_cols.append(close[0] if close else col)\n",
    "            else:\n",
    "                fallback_cols.append(col)\n",
    "\n",
    "        # Attempt resolving computed columns like 'A + B'\n",
    "        resolved_cols = resolve_computed_columns(fallback_cols, df)\n",
    "\n",
    "        # Final column check\n",
    "        if not all(col in df.columns for col in resolved_cols):\n",
    "            print(f\"❌ Still unmatched columns: {resolved_cols}\")\n",
    "            continue\n",
    "        else:\n",
    "            cols = resolved_cols\n",
    "        \n",
    "        try:\n",
    "            # ✅ Display title and explanation only (handler shows chart)\n",
    "            display(HTML(f\"<h5>✅ {title}</h5>\"))\n",
    "            \n",
    "            # Dispatch to the appropriate plotting function\n",
    "            plot_func = plot_dispatch.get(chart_type)\n",
    "            if plot_func:\n",
    "                plot_func(df, cols, title)\n",
    "                if explanation:\n",
    "                    display(HTML(f\"<p><b>Explanation:</b> {explanation}</p>\"))\n",
    "                any_plotted = True\n",
    "            else:\n",
    "                print(f\"⚠️ Unsupported chart type: {chart_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error plotting {title}: {e}\")\n",
    "\n",
    "    if not any_plotted:\n",
    "        display(HTML(\"<b>⚠️ No valid plots were created from LLM suggestions.</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab99ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968a12801b79407ca3121d9d3aa76a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "output_area = widgets.Output()\n",
    "display(output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a55f35c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>📁 Upload Your CSV Dataset:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf17ceeb61064102b67eca47eac9361f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.csv', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb53ccb441942c9ac55957e46c00952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='📤 Submit File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "upload = widgets.FileUpload(accept='.csv', multiple=False)\n",
    "submit_button = widgets.Button(description=\"📤 Submit File\")\n",
    "\n",
    "def handle_upload(change):\n",
    "    global df\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"📥 Upload event triggered.\")\n",
    "        if upload.value:\n",
    "            try:\n",
    "                uploaded_file = upload.value[0]\n",
    "                content = uploaded_file['content']\n",
    "                df = pd.read_csv(BytesIO(content))\n",
    "                globals()['df_raw'] = df.copy()  # ✅ Store original uncleaned version\n",
    "                print(\"✅ File read into DataFrame.\")\n",
    "                \n",
    "                # Auto-parse datetime columns\n",
    "                for col in df.columns:\n",
    "                    if df[col].dtype == 'object':\n",
    "                        try:\n",
    "                            df[col] = pd.to_datetime(df[col])\n",
    "                            print(f\"📅 Converted '{col}' to datetime.\")\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                df_cleaned = df.dropna().drop_duplicates()\n",
    "                globals()[\"df_cleaned\"] = df_cleaned\n",
    "                \n",
    "                df = df_cleaned.copy()\n",
    "\n",
    "                display(HTML(\"<h4>✅ Dataset Uploaded and Cleaned</h4>\"))\n",
    "                display(df_cleaned.head())\n",
    "\n",
    "                suggestions = generate_plot_suggestions(df_cleaned)\n",
    "                instructions = extract_plot_instructions(suggestions)\n",
    "                generate_plots_from_instructions(df_cleaned, instructions)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in handle_upload: {e}\")\n",
    "\n",
    "submit_button.on_click(handle_upload)\n",
    "display(HTML(\"<h3>📁 Upload Your CSV Dataset:</h3>\"))\n",
    "display(upload, submit_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2aac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 💬 Ask a Question About the Dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc65577036542b39bc30e0494cd58aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='🧠 Query:', layout=Layout(height='80px', width='100%'), placeholder='Ask a ques…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aec8ee65564afdbb20cf76118c8a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Use Cleaned Dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8546f0afb974457b031e59000ccfe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Answer Query', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1592a6efc53448d1b135ae235c0e27f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import traceback\n",
    "import re\n",
    "import json\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# UI\n",
    "use_cleaned_toggle = widgets.Checkbox(value=True, description='Use Cleaned Dataset')\n",
    "query_input = widgets.Textarea(\n",
    "    placeholder='Ask a question about the dataset...',\n",
    "    description='🧠 Query:',\n",
    "    layout=widgets.Layout(width='100%', height='80px'),\n",
    ")\n",
    "\n",
    "submit_query = widgets.Button(description=\"Answer Query\", button_style=\"primary\")\n",
    "output_query = widgets.Output()\n",
    "\n",
    "\n",
    "display(Markdown(\"### 💬 Ask a Question About the Dataset\"))\n",
    "display(query_input, use_cleaned_toggle, submit_query, output_query)\n",
    "\n",
    "# Handler function\n",
    "def handle_query(_):\n",
    "    output_query.clear_output()\n",
    "    user_question = query_input.value.strip()\n",
    "\n",
    "    if not user_question:\n",
    "        with output_query:\n",
    "            print(\"❗ Please enter a question.\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a Python data analyst working with a pandas DataFrame named `df`.\n",
    "\n",
    "The user asked:\n",
    "\\\"\\\"\\\"{user_question}\\\"\\\"\\\"\n",
    "\n",
    "Your job:\n",
    "1. Classify the query as either:\n",
    "   - \"data\": a statistical question (e.g. average, count, percentage)\n",
    "   - \"chart\": a visual question (e.g. plot, graph, chart)\n",
    "\n",
    "2. If the type is \"data\", return a **single-line** Python expression (like df['col'].mean()) that evaluates the answer using the DataFrame `df`.\n",
    "\n",
    "3. If the type is \"chart\", return Python code using matplotlib or seaborn to plot the requested visualization using the DataFrame `df`.\n",
    "\n",
    "NOTE: When generating charts:\n",
    "- Always **group and aggregate** data properly (e.g. use `.groupby()` and `.mean()` or `.sum()`).\n",
    "- Avoid multi-index plots (like `('Male', 0)`); instead, pivot or aggregate to keep axes simple.\n",
    "- Convert `.value_counts()` or `.groupby()` results to `.plot(kind='bar')` where appropriate.\n",
    "- Use proper axis labels and titles.\n",
    "- When the user asks about “top N ages,” interpret that as individual age values, not binned ranges (unless “age groups” is explicitly mentioned).\n",
    "\n",
    "4. Your full response must be valid JSON using this format:\n",
    "{{\n",
    "  \"type\": \"data\" or \"chart\",\n",
    "  \"result\": \"python code or expression here\"\n",
    "}}\n",
    "\n",
    "No explanation. Only return a valid JSON object.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "        You are a data analyst writing Python code to answer questions about a pandas DataFrame named `df`.\n",
    "\n",
    "        Always follow this logic:\n",
    "\n",
    "        1. If the user asks for a statistical answer:\n",
    "        - Respond with a Python expression like: df['Survived'].mean()\n",
    "\n",
    "        2. If the user asks for a chart:\n",
    "        - Use matplotlib or seaborn\n",
    "        - Group data meaningfully. Do NOT use `.value_counts()` for numeric fields\n",
    "        - If query mentions 'age group', use pd.cut(df['Age'], bins=[...]) to bin ages\n",
    "        - Label all axes and add titles\n",
    "        - Use: df.groupby(), df.pivot_table(), or pd.cut()\n",
    "\n",
    "        Only output this JSON format:\n",
    "        {\n",
    "        \"type\": \"data\" or \"chart\",\n",
    "        \"result\": \"python code or expression here as a string\"\n",
    "        }\n",
    "        \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_question\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )        \n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        try:\n",
    "            result_json = json.loads(reply)\n",
    "        except json.JSONDecodeError:\n",
    "            with output_query:\n",
    "                print(\"❌ Could not parse LLM output as JSON:\")\n",
    "                print(reply)\n",
    "            return\n",
    "\n",
    "        result_type = result_json.get(\"type\")\n",
    "        result_value = result_json.get(\"result\")\n",
    "\n",
    "        with output_query:\n",
    "            if \"df\" not in globals():\n",
    "                print(\"❌ Global DataFrame `df` not found. Please upload the dataset first.\")\n",
    "                return\n",
    "\n",
    "            if result_type == \"data\":\n",
    "                print(\"🧮 Evaluating:\", result_value)\n",
    "                try:\n",
    "                    result = eval(result_value, globals(), {\"df\": df if use_cleaned_toggle.value else df_raw})\n",
    "                    \n",
    "                    if isinstance(result, (int, float)):\n",
    "                        # Single value\n",
    "                        data_used = \"cleaned\" if use_cleaned_toggle.value else \"original\"\n",
    "                        display(Markdown(f\"📊 **Using `{data_used}` dataset**\"))\n",
    "                        display(Markdown(f\"📝 **Answer:**\\n\\n{user_question} → **{result:.2f}**\"))\n",
    "                    \n",
    "                    elif isinstance(result, pd.Series):\n",
    "                        # Series → formatted table\n",
    "                        markdown_table = \"\\n\".join(\n",
    "                            [f\"- **{idx}**: {val}\" for idx, val in result.items()]\n",
    "                        )\n",
    "                        data_used = \"cleaned\" if use_cleaned_toggle.value else \"original\"\n",
    "                        display(Markdown(f\"📊 **Using `{data_used}` dataset**\"))\n",
    "                        display(Markdown(f\"📝 **Answer:**\\n\\n**{user_question}**\\n\\n{markdown_table}\"))\n",
    "                    \n",
    "                    elif isinstance(result, pd.DataFrame):\n",
    "                        # Display a simple HTML table\n",
    "                        data_used = \"cleaned\" if use_cleaned_toggle.value else \"original\"\n",
    "                        display(Markdown(f\"📊 **Using `{data_used}` dataset**\"))\n",
    "                        display(Markdown(f\"📝 **Answer:**\\n\\n**{user_question}**\"))\n",
    "                        display(result.head())\n",
    "\n",
    "                    else:\n",
    "                        # Other types\n",
    "                        data_used = \"cleaned\" if use_cleaned_toggle.value else \"original\"\n",
    "                        display(Markdown(f\"📊 **Using `{data_used}` dataset**\"))\n",
    "                        display(Markdown(f\"📝 **Answer:**\\n\\n{user_question} → **{result}**\"))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"❌ Error evaluating code:\")\n",
    "                    traceback.print_exc()\n",
    "                \n",
    "            elif result_type == \"chart\":\n",
    "                print(\"📊 Executing Chart Code:\\n\", result_value)\n",
    "                try:\n",
    "                    # Patch: Auto-correct column case in code using actual df.columns\n",
    "                    actual_cols = list(df.columns)\n",
    "                    for col in actual_cols:\n",
    "                        lower_col = col.lower()\n",
    "                        pattern = rf\"\\['{lower_col}'\\]\"\n",
    "                        corrected = f\"['{col}']\"\n",
    "                        result_value = re.sub(pattern, corrected, result_value, flags=re.IGNORECASE)\n",
    "\n",
    "                    exec(result_value, {\"df\": df, \"pd\": pd, \"plt\": plt, \"sns\": sns})\n",
    "                except Exception:\n",
    "                    print(\"❌ Error running chart code:\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(\"❌ Unknown type returned:\", result_type)\n",
    "\n",
    "    except Exception:\n",
    "        with output_query:\n",
    "            print(\"❌ Error during LLM request:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "submit_query.on_click(handle_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785b0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 🧠 Train a Machine Learning Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ee83c8b2f4484385539b32e705403c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='📊 Model Query:', layout=Layout(height='100px', width='100%'), placeholder='Des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9049875a055461eab053cc328d92daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Type:', options=('classification', 'regression'), value='classification')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c756396a9c904946817053bdd66ecdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Train Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0aee08ab0d4429bf06a2788c096d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='💾 Export Model', disabled=True, style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe41852644d4155bc055e727ff0a352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import traceback\n",
    "import json\n",
    "import ast\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# === UI Elements ===\n",
    "model_query_input = widgets.Textarea(\n",
    "    placeholder='Describe the model you want to train (e.g., \"Train a classification model to predict Survived from Age, Sex, and Pclass\")',\n",
    "    description='📊 Model Query:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    ")\n",
    "\n",
    "model_type_dropdown = widgets.Dropdown(\n",
    "    options=['classification', 'regression'],\n",
    "    value='classification',\n",
    "    description='Model Type:',\n",
    ")\n",
    "\n",
    "submit_model_btn = widgets.Button(description=\"Train Model\", button_style=\"success\")\n",
    "export_model_btn = widgets.Button(description=\"💾 Export Model\", disabled=True)\n",
    "model_output = widgets.Output()\n",
    "\n",
    "# === Display UI ===\n",
    "display(Markdown(\"### 🧠 Train a Machine Learning Model\"))\n",
    "display(model_query_input, model_type_dropdown, submit_model_btn, export_model_btn, model_output)\n",
    "\n",
    "# === Handlers ===\n",
    "def handle_model_training(_):\n",
    "    model_output.clear_output()\n",
    "    query = model_query_input.value.strip()\n",
    "    if not query:\n",
    "        with model_output:\n",
    "            print(\"❗ Please enter a training request.\")\n",
    "        return\n",
    "\n",
    "    model_type = model_type_dropdown.value\n",
    "    df_active = df if use_cleaned_toggle.value else df_raw\n",
    "\n",
    "    # Prepare column names\n",
    "    column_names = list(df.columns)\n",
    "    column_dtypes = df.dtypes.astype(str).to_dict()\n",
    "    missing_perc = df.isnull().mean().round(2).to_dict()\n",
    "    \n",
    "    column_info_str = \"\\n\".join(\n",
    "    f\"- {col} (type: {column_dtypes[col]}, missing: {missing_perc[col]*100:.0f}%)\"\n",
    "    for col in column_names\n",
    ")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a Python data scientist. Given the user's request:\n",
    "\\\"\\\"\\\"{query}\\\"\\\"\\\"\n",
    "\n",
    "The available columns in the dataset are:\n",
    "{column_info_str}\n",
    "\n",
    "Generate Python code to train a {model_type} model using scikit-learn. The DataFrame is `df`.\n",
    "\n",
    "You must:\n",
    "- Dynamically select target and features based on the query\n",
    "- Handle categorical columns using LabelEncoder or get_dummies\n",
    "- Drop or ignore datetime columns if needed\n",
    "- Only use columns that exist in the dataset.\n",
    "- Do NOT invent or assume any column names like \"Signup_Date\", \"User_ID\", etc.\n",
    "- Before selecting features, verify they exist in the list above.\n",
    "- Avoid columns with >50% missing values unless explicitly requested.\n",
    "- Drop rows with missing values, especially in the target column\n",
    "- Before training:\n",
    "    - Check if the final dataset has at least 2 rows\n",
    "    - If not, raise: `raise ValueError(\"Insufficient data after cleaning. Please choose different features or handle missing values.\")`\n",
    "- Train the model (e.g., RandomForestClassifier or LinearRegression)\n",
    "- Evaluate (use accuracy or R^2 score)\n",
    "- Print feature importances if supported\n",
    "- Assign the trained model to a variable named `model`\n",
    "\n",
    "Return only valid JSON using this format:\n",
    "{{\n",
    "  \"model_type\": \"{model_type}\",\n",
    "  \"code\": \\\"\\\"\\\"<Python training code here>\\\"\\\"\\\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You return valid JSON with model training code.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(reply)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                parsed = ast.literal_eval(reply)\n",
    "            except Exception:\n",
    "                with model_output:\n",
    "                    print(\"❌ Could not parse model code:\")\n",
    "                    print(reply)\n",
    "                return\n",
    "\n",
    "        model_code = parsed.get(\"code\")\n",
    "\n",
    "        with model_output:\n",
    "            print(\"🚀 Training model with the following code:\\n\")\n",
    "            print(model_code)\n",
    "            print(\"\\n===============================\\n\")\n",
    "\n",
    "            local_vars = {\"df\": df_active, \"pd\": pd, \"plt\": plt, \"sns\": sns}\n",
    "            exec(model_code, local_vars)\n",
    "\n",
    "            if 'model' in local_vars:\n",
    "                export_model_btn.disabled = False\n",
    "                export_model_btn.model = local_vars['model']\n",
    "\n",
    "    except Exception as e:\n",
    "        with model_output:\n",
    "            print(\"❌ Error during model generation or training:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "def handle_export_model(_):\n",
    "    if hasattr(export_model_btn, 'model'):\n",
    "        try:\n",
    "            joblib.dump(export_model_btn.model, 'trained_model.pkl')\n",
    "            with model_output:\n",
    "                print(\"✅ Model exported as 'trained_model.pkl'\")\n",
    "        except Exception as e:\n",
    "            with model_output:\n",
    "                print(\"❌ Failed to export model:\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "submit_model_btn.on_click(handle_model_training)\n",
    "export_model_btn.on_click(handle_export_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8faec37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in ./.venv/lib/python3.13/site-packages (from streamlit) (5.5.2)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in ./.venv/lib/python3.13/site-packages (from streamlit) (2.3.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in ./.venv/lib/python3.13/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in ./.venv/lib/python3.13/site-packages (from streamlit) (2.3.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in ./.venv/lib/python3.13/site-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in ./.venv/lib/python3.13/site-packages (from streamlit) (5.29.5)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./.venv/lib/python3.13/site-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from streamlit) (9.1.2)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in ./.venv/lib/python3.13/site-packages (from streamlit) (4.14.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in ./.venv/lib/python3.13/site-packages (from streamlit) (6.5.1)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.46.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.26.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m731.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m863.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m588.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading narwhals-1.46.0-py3-none-any.whl (373 kB)\n",
      "Downloading pyarrow-20.0.0-cp313-cp313-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.26.0-cp313-cp313-macosx_11_0_arm64.whl (350 kB)\n",
      "Installing collected packages: toml, smmap, rpds-py, pyarrow, narwhals, MarkupSafe, click, blinker, referencing, jinja2, gitdb, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [streamlit]17\u001b[0m [streamlit]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 altair-5.5.0 blinker-1.9.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.44 jinja2-3.1.6 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 narwhals-1.46.0 pyarrow-20.0.0 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.26.0 smmap-5.0.2 streamlit-1.46.1 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angad.sharma@zomato.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
